{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3675224",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Surface Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905aeca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------#------------#\n",
    "# Get them Libraries\n",
    "#------------#------------#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pathlib\n",
    "\n",
    "import matplotlib as mpl\n",
    "from scipy.ndimage import label\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "#------------#------------#\n",
    "# Font Settings\n",
    "#------------#------------#\n",
    "\n",
    "# Set global font to Times New Roman and font size to 10pt\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 10\n",
    "plot_font_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75afe257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------#------------#\n",
    "# File Names and Paths\n",
    "#------------#------------#\n",
    "\n",
    "# Did it Gall?? ## Only use for running in this file (not from main)\n",
    "#did_it_gall = False\n",
    "\n",
    "# Folder Name ## Only use for running in this file (not from main)\n",
    "#xyz_folder_name = \"A - 316L Rod\"\n",
    "\n",
    "# File Names ## Only use for running in this file (not from main)\n",
    "#sample_name = 'A1 Pre-Galling Test.xyz'\n",
    "\n",
    "# Code File Location\n",
    "code_dir = os.getcwd()\n",
    "\n",
    "# Source File relative to code file\n",
    "csv_path = os.path.join(code_dir, \"..\", \"xyz Files\", xyz_folder_name, sample_name)\n",
    "\n",
    "# Replace the file extension for the output CSV file\n",
    "saving_h5_name = sample_name.replace('.xyz', '_processed.h5')\n",
    "\n",
    "# Output Folder\n",
    "target_folder = xyz_folder_name\n",
    "\n",
    "# Find the file path within the target folder\n",
    "saving_h5_path = os.path.join(code_dir, \"..\", \"Processed Files\", target_folder, saving_h5_name)\n",
    "\n",
    "# Quantification file name\n",
    "quantification_file_name = 'Surface Quantification Values.txt'\n",
    "\n",
    "# Find the file path to save\n",
    "quantification_file_path = os.path.join(code_dir, \"..\", \"Processed Files\", quantification_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47166ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check parameter selecetion, skip_rows =  3\n",
      "Edge size / mm:  0.1\n"
     ]
    }
   ],
   "source": [
    "#------------#------------#\n",
    "# Profilometer Scan Parameters\n",
    "#------------#------------#\n",
    "\n",
    "%run Profilometer_Scan_Parameters.ipynb {csv_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e5c7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name is:  /Users/kkir0008/Desktop/PhD/Imaging/Optical Profilometer/Profilometry Code/../Processed Files/A - 316L Rod/A1 Pre-Galling Test_processed.h5\n",
      "Sq / - : 2.087844138755609\n",
      "Ssk / um: -28.959705858558223\n",
      "Mean of the array: 0.6632 µm\n",
      "Squared differences from the mean: [18.47167131 19.48811992 19.92702073 ...  2.09787467  1.82031927\n",
      "  1.60141478]\n",
      "Mean of the squared differences: 28.49 µm²\n",
      "The RMS value of the 2D array is: 5.337 µm\n",
      "min angle:  -3.1408327755208876  max angle:  3.141592653589793\n",
      "Duplicate entry for file_name '/Users/kkir0008/Desktop/PhD/Imaging/Optical Profilometer/Profilometry Code/../Processed Files/A - 316L Rod/A1 Pre-Galling Test_processed.h5' found and removed before appending new data.\n",
      "Time elapsed:  2.946150064468384  seconds\n"
     ]
    }
   ],
   "source": [
    "#------------#------------#\n",
    "# Surface Quantification\n",
    "#------------#------------#\n",
    "\n",
    "\n",
    "#------------#------------#\n",
    "# Data File\n",
    "#------------#------------#\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def check_and_write_headers(file_path, headers):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        # If file does not exist, create it and write the headers\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(', '.join(headers) + '\\n')\n",
    "    else:\n",
    "        # If file exists, check for missing headers\n",
    "        with open(file_path, 'r+') as file:\n",
    "            existing_headers = file.readline().strip().split(',')\n",
    "            new_headers = [header for header in headers if header not in existing_headers]\n",
    "            \n",
    "            if new_headers:\n",
    "                # Move to start of file and add new headers\n",
    "                file.seek(0)\n",
    "                updated_headers = existing_headers + new_headers\n",
    "                file.write(','.join(updated_headers) + '\\n')\n",
    "                \n",
    "                # Read rest of the data, skip first line, and rewrite the file\n",
    "                data = file.readlines()[1:]\n",
    "                file.seek(0)\n",
    "                file.write(','.join(updated_headers) + '\\n')\n",
    "                file.writelines(data)\n",
    "\n",
    "\n",
    "headers = ['File Name', 'Processing Date', 'Sq', 'Ssk (µm)', 'Mean Height (µm)', 'Max Height (μm)', 'Mean of Squares (µm²)', \n",
    "           'RSM (µm)', 'Sku (Kurtosis)', 'Displaced Volume (mm^3)',\n",
    "           \n",
    "           'Trough area (%)', 'Peak area (%)', 'Galled area (%)', 'Trough Volume (mm^3)', \n",
    "           'Peak Volume (mm^3)', 'Galled Volume (mm^3)', 'Average Galled Radius (mm)', \n",
    "           'Gall Track Width (mm)', 'Swept Galled Angle (˚)']\n",
    "\n",
    "check_and_write_headers(quantification_file_path, headers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def quanfitication(saving_h5_path):\n",
    "\n",
    "#------------#------------#\n",
    "# Load Data\n",
    "#------------#------------#\n",
    "    print('File name is: ', saving_h5_path)\n",
    "# Load data from HDF5 file\n",
    "    with h5py.File(saving_h5_path, 'r') as f:\n",
    "        X = f['X_new'][:]\n",
    "        Y = f['Y_new'][:]\n",
    "        pixel_size = f.attrs['pixel_size']\n",
    "        data = pd.DataFrame(f['translated_data'][:])\n",
    "        raw = pd.DataFrame(f['raw_data'][:])\n",
    "        tilted_data = pd.DataFrame(f['tilted_data_3'][:])\n",
    "\n",
    "# Resolution\n",
    "    X_res = X[1] - X[0] # mm\n",
    "    Y_res = Y[1] - Y[0] # mm\n",
    "    mean_res = (X_res + Y_res)/2 # mm\n",
    "    \n",
    "# Generate z_clean\n",
    "    z_clean = data.to_numpy().flatten()[~np.isnan(data.to_numpy().flatten())]\n",
    "# Generate Z_sort\n",
    "    Z_sort_nan = np.sort(np.ndarray.flatten(np.asarray(data))) #Flattening the dataframe into a 1D array & ordering (doesn't matter but easy to do)\n",
    "    Z_sort = Z_sort_nan[~np.isnan(Z_sort_nan)] #Removing the NaN cells (enabling further operations to be performed), -ve sign to remove NaN\n",
    "\n",
    "\n",
    "#------------#------------#\n",
    "# Quantify Bulk Surface data\n",
    "#------------#------------#\n",
    "\n",
    "#Finding Sq and Ssk (quantification of data skew)\n",
    "\n",
    "    Z2 = Z_sort**2\n",
    "    Z3 = Z_sort**3\n",
    "\n",
    "    R_Area = (1000*X_res)*(1000*Y_res)*len(Z_sort) #Multiplied by 1000 (twice) since Z is in um andd X/Ynewsize are in mm\n",
    "\n",
    "\n",
    "    Sq = np.sqrt(np.sum(Z2)/R_Area)\n",
    "    print('Sq / - :', Sq)\n",
    "\n",
    "    Ssk = (np.sum(Z3)/R_Area)/(Sq**3)\n",
    "    print('Ssk / um:', Ssk)\n",
    "\n",
    "\n",
    "# Calculate the mean (Sa)\n",
    "    mean_value = Z_sort.mean()\n",
    "    print(f\"Mean of the array (Sa): {mean_value:.4g} µm\")\n",
    "\n",
    "# Calculate differences from the mean\n",
    "    differences = (z_clean - mean_value)\n",
    "\n",
    "# Calculate the max height\n",
    "    max_height = max(differences) - min(differences)\n",
    "    \n",
    "# Calculate squared differences from the mean\n",
    "    squared_differences = np.square(z_clean - mean_value)\n",
    "    #print(\"Squared differences from the mean:\", squared_differences)\n",
    "\n",
    "# Calculate the mean of the squared differences\n",
    "    mean_of_squared_differences = np.mean(squared_differences)\n",
    "    print(f\"Mean of the squared differences: {mean_of_squared_differences:.4g} µm²\")\n",
    "\n",
    "# Calculate the RMS value\n",
    "    rms_value = np.sqrt(mean_of_squared_differences)\n",
    "    print(f\"The RMS value of the 2D array is: {rms_value:.4g} µm\")\n",
    "\n",
    "# Calculate the Sku (Kurtosis)\n",
    "    Sku = (1/rms_value**4) * np.sqrt(np.sqrt(mean(np.square(np.square(z_clean - mean_value))))) # um\n",
    "    \n",
    "# Displaced Volume\n",
    "    DV = X_res*Y_res*sum(abs(Z_sort))/1000 # mm^3\n",
    "    \n",
    "# Aspect Ratio needs polar coordinates to really mean anything\n",
    "    #aspect_ratio =  # no units\n",
    "    \n",
    "#------------#------------#\n",
    "# Quantify Galled Surface data\n",
    "#------------#------------#\n",
    "    if did_it_gall:\n",
    "# Set a threshold range around the median\n",
    "        threshold = 10  # um Adjust this value to your needs\n",
    "    else:\n",
    "        threshold = 1  # um Adjust this value to your needs\n",
    "\n",
    "# Recalculate cumulative counts\n",
    "    cumulative_counts = np.arange(1, len(Z_sort) + 1)\n",
    "# Convert counts to normalized percentages\n",
    "    percentage_below = cumulative_counts / len(Z_sort)\n",
    "\n",
    "    lower_bound = -threshold\n",
    "    upper_bound = threshold\n",
    "    \n",
    "# Filter values outside the acceptable range\n",
    "    below_lower_bound_values = Z_sort[Z_sort < lower_bound]\n",
    "    above_upper_bound_values = Z_sort[Z_sort > upper_bound]\n",
    "\n",
    "# Calculate how many points are outside the acceptable range\n",
    "    points_below_lower_bound = len(below_lower_bound_values)\n",
    "    points_above_upper_bound = len(above_upper_bound_values)\n",
    "\n",
    "    percentage_below_lower_bound = points_below_lower_bound / len(Z_sort) * 100\n",
    "    percentage_above_upper_bound = points_above_upper_bound / len(Z_sort) * 100\n",
    "    percentage_out_of_bounds = percentage_below_lower_bound + points_above_upper_bound\n",
    "\n",
    "# Volume calculation for points in mm^3\n",
    "    volume_below_lower_bound = np.sum(below_lower_bound_values/1000 * pixel_size)\n",
    "    volume_above_upper_bound = np.sum(above_upper_bound_values/1000 * pixel_size)\n",
    "    volume_out_of_bounds = volume_below_lower_bound + volume_above_upper_bound\n",
    "\n",
    "#Finding the sample area and galled area\n",
    "    sample_area = math.pi*(outer_radius**2 - inner_radius**2) #From ASTM G196 sample drawing\n",
    "    trough_area = points_below_lower_bound * pixel_size\n",
    "    peak_area = points_above_upper_bound * pixel_size\n",
    "    galled_area = trough_area + peak_area\n",
    "    \n",
    "# Radius of Galled Area\n",
    "    galled_data = np.where(pd.isna(data[data < lower_bound]), data[data > upper_bound], data[data < lower_bound] + np.nan_to_num(data[data > upper_bound]))  \n",
    "    X_grid, Y_grid = np.meshgrid(X, Y)\n",
    "    X_0 = np.where(X == 0)[0]\n",
    "    Y_0 = np.where(Y == 0)[0]\n",
    "    radius_data = np.sqrt(((X_grid - X_0) * X_res)**2 + ((Y_grid - Y_0) * Y_res)**2);\n",
    "    mask = np.isnan(galled_data)\n",
    "    radius_data = radius_data[~mask] # it auto converts to a 1D array??\n",
    "    average_galled_radius = np.mean(radius_data)\n",
    "# Length and Width of galled region\n",
    "    min_radius = radius_data.min().min()\n",
    "    max_radius = radius_data.max().max()\n",
    "    gall_track_width = max_radius - min_radius\n",
    "\n",
    "        \n",
    "# Angle of Galled Area\n",
    "    # Find points where height data is either above +threshold or below -threshold\n",
    "    binary_mask = (data >= threshold) | (data <= -threshold)\n",
    "\n",
    "# Label the connected components\n",
    "    labeled, num_features = label(binary_mask)\n",
    "\n",
    "# Optionally, filter out small features (e.g., islands)\n",
    "    sizes = np.bincount(labeled.ravel())\n",
    "\n",
    "    if len(sizes) < 3:\n",
    "        print(\"Not enough components to find the second largest.\")\n",
    "        min_size = 100000  # Minimum size of connected component (in terms of number of pixels)\n",
    "        max_size = 5000000\n",
    "        mask_size = (sizes >= min_size) & (sizes <= max_size)\n",
    "        valid_components = mask_size[labeled]\n",
    "    else:\n",
    "        largest_label = np.argmax(sizes[1:]) + 1  # Find the largest component label, ignoring background\n",
    "        sizes[largest_label] = 0  # Temporarily set largest size to zero to find the second largest\n",
    "        second_largest_label = np.argmax(sizes[1:]) + 1  # Find second-largest component label\n",
    "    # Create a mask for the second-largest component\n",
    "        second_largest_component = (labeled == second_largest_label) \n",
    "        valid_components = second_largest_component ### second_largest_component[labeled] ---- something in here may have killed the kernal ----####\n",
    "\n",
    "# Now valid_components contains only the large connected regions\n",
    "    valid_points = np.argwhere(valid_components)\n",
    "\n",
    "# Assuming valid_points is an array of shape (N, 2) representing coordinates\n",
    "    x_points = X[valid_points[:, 1]]\n",
    "    y_points = Y[valid_points[:, 0]]\n",
    "    curve_points = np.column_stack((x_points, y_points))\n",
    "\n",
    "# Angle-based sorting for roughly circular/continuous curve\n",
    "    centroid = [0,0]#np.mean(curve_points, axis = 0)\n",
    "    angles = np.degrees(np.arctan2(curve_points[:, 1] - centroid[1], curve_points[:, 0] - centroid[0]))\n",
    "    sorted_indices = np.argsort(angles)\n",
    "    ordered_curve_points = curve_points[sorted_indices]\n",
    "\n",
    "    min_angle = np.min(angles)\n",
    "    max_angle = np.max(angles)\n",
    "    angle_range = max_angle - min_angle\n",
    "\n",
    "    if angle_range > 3.1:\n",
    "# Split points into two halves based on `y = 0` relative to centroid\n",
    "        upper_half = angles[curve_points[:, 1] >= centroid[1]]\n",
    "        lower_half = angles[curve_points[:, 1] < centroid[1]]\n",
    "\n",
    "# Calculate min and max angles for each half\n",
    "        upper_min, upper_max = upper_half.min(), upper_half.max()\n",
    "        lower_min, lower_max = lower_half.min(), lower_half.max()\n",
    "        angle_range = upper_max - upper_min + upper_max - upper_min\n",
    "\n",
    "    print(\"min angle: \", min_angle, \" max angle: \", max_angle)\n",
    "\n",
    "#------------#------------#\n",
    "# Save quantified data\n",
    "#------------#------------#\n",
    "\n",
    "    file_name = os.path.basename(saving_h5_path)\n",
    "    file_path = pathlib.Path(saving_h5_path)\n",
    "    date = os.path.getmtime(file_path)\n",
    "\n",
    "    \n",
    "    \n",
    "    quantified_data = [file_name, date, Sq, Ssk, mean_value, max_height, mean_of_squared_differences, \n",
    "                       rms_value, percentage_below_lower_bound, percentage_above_upper_bound, \n",
    "                       percentage_out_of_bounds, average_galled_radius, gall_track_width, \n",
    "                       angle_range]\n",
    "\n",
    "    def append_data(file_path, data, headers):\n",
    "# Load existing lines and check for duplicates of 'file_name' value\n",
    "        file_name_index = headers.index('File Name')\n",
    "        new_file_name = data[file_name_index]\n",
    "    \n",
    "# Read file data\n",
    "        lines_to_keep = []\n",
    "        duplicate_found = False\n",
    "    \n",
    "# Read all lines and filter out duplicates\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                lines = file.readlines()\n",
    "                header_line = lines[0]  # Preserve headers\n",
    "                for line in lines[1:]:\n",
    "                    line_data = line.strip().split(', ')\n",
    "                    if line_data[file_name_index] == new_file_name:\n",
    "                        duplicate_found = True\n",
    "                    else:\n",
    "                        lines_to_keep.append(line)\n",
    "\n",
    "# Write back the file without duplicates\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(header_line)  # Write header line\n",
    "            file.writelines(lines_to_keep)  # Write non-duplicate data lines\n",
    "            file.write(', '.join(map(str, data)) + '\\n')  # Append the new data\n",
    "\n",
    "        if duplicate_found:\n",
    "            print(f\"Duplicate entry for file_name '{new_file_name}' found and removed before appending new data.\")\n",
    "\n",
    "    append_data(quantification_file_path, quantified_data, headers)\n",
    "\n",
    "\n",
    "quanfitication(saving_h5_path)\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time elapsed: ', end_time - start_time, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbca98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
