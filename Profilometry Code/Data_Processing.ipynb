{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2c0f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cca2aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------#------------#\n",
    "# Get them Libraries\n",
    "#------------#------------#\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "import math\n",
    "from scipy import interpolate\n",
    "from statistics import mean\n",
    "\n",
    "import h5py\n",
    "\n",
    "#------------#------------#\n",
    "# Font Settings\n",
    "#------------#------------#\n",
    "\n",
    "# Set global font to Times New Roman and font size to 10pt\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 10\n",
    "plot_font_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------#------------#\n",
    "# File Names and Paths\n",
    "#------------#------------#\n",
    "\n",
    "\n",
    "# Folder Name ## Only use for running in this file (not from main)\n",
    "#xyz_folder_name = \"A - 316L Rod\"\n",
    "\n",
    "# File Names ## Only use for running in this file (not from main)\n",
    "#sample_name = 'A1 Pre-Galling Test.xyz'\n",
    "\n",
    "# Code File Location\n",
    "code_dir = os.getcwd()\n",
    "\n",
    "# Source File relative to code file\n",
    "csv_path = os.path.join(code_dir, \"..\", \"xyz Files\", xyz_folder_name, sample_name)\n",
    "\n",
    "# Create the output file name based on the input file name\n",
    "output_name = sample_name.replace('.xyz', '_piv.txt')\n",
    "\n",
    "# Replace the file extension for the output CSV file\n",
    "saving_h5_name = sample_name.replace('.xyz', '_processed.h5')\n",
    "\n",
    "# Output Folder\n",
    "target_folder = xyz_folder_name\n",
    "\n",
    "# Find the file path within the target folder\n",
    "output_path = os.path.join(code_dir, \"..\", \"Processed Files\", target_folder, output_name)\n",
    "\n",
    "\n",
    "# Find the file path within the target folder\n",
    "saving_h5_path = os.path.join(code_dir, \"..\", \"Processed Files\", target_folder, saving_h5_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eb4945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check parameter selection, skip_rows =  3\n",
      "Edge size / mm:  0.1\n"
     ]
    }
   ],
   "source": [
    "#------------#------------#\n",
    "# Profilometer Scan Parameters\n",
    "#------------#------------#\n",
    "\n",
    "%run Profilometer_Scan_Parameters.ipynb {csv_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "003e64df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the CSV file\n",
      "Pivoted the DataFrame\n",
      "Read, Convert, and Save time: 10.21 seconds\n",
      "Total Filtering Time (s):  47.218563079833984\n",
      "Interpolation Time (s):  67.81396889686584\n",
      "Time for first tilt correction (s):  70.96553587913513\n",
      "Time for third tilt correction (s):  77.06639385223389\n",
      "Position of modal bin (counts):  92 - 94\n",
      "Z-correction factor:  93.0\n",
      "Pixel size is 6.636e-12 mm^2\n",
      "Time for full function (s):  79.82726693153381\n",
      "Finished!\n",
      "Time elapsed:  79.88720393180847  seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def process_raw_data(file_path, output_path, saving_h5_path):\n",
    "\n",
    "#------------#------------#\n",
    "# Step 0: Define Analysis Parameters\n",
    "#------------#------------#\n",
    "\n",
    "# Filtering parameters\n",
    "    moving_average = 15 - 1 # must be even?????\n",
    "    std_range = 1 #Number of standard deviations from mean which is accepted\n",
    "\n",
    "\n",
    "# Min data points in a row/column for physical centre locating\n",
    "    min_points = 250 # Minimum number of data points in a row/column that we'll consider as being a relevant row/column for calculating the mid-point\n",
    "\n",
    "\n",
    "#------------#------------#\n",
    "# Step 1: Read, convert, and save\n",
    "#------------#------------#\n",
    "\n",
    "# Read the CSV file\n",
    "    unprocessed_data = pd.read_csv(file_path, sep=col_sep, skiprows=skip_rows, usecols=[0,1,2])\n",
    "    print('Read the CSV file')\n",
    "    \n",
    "# Remove last row (if erroneous)\n",
    "    if not isinstance(unprocessed_data.iloc[-1, 0], (int, float)):\n",
    "        unprocessed_data = unprocessed_data.iloc[:-1]\n",
    "        \n",
    "# Replace \"No\" with NaN\n",
    "    if unprocessed_data.iloc[0,-1] == \"No\":\n",
    "        unprocessed_data.replace(\"No\", np.nan, inplace=True)\n",
    "        \n",
    "# Assuming the DataFrame has the structure [X, Y, Z]\n",
    "    unprocessed_data.columns = ['X', 'Y', 'Z']  # Assign column names if needed\n",
    "    unprocessed_data['X'] = unprocessed_data['X'].astype('float64') *unit_conversion_xy\n",
    "    unprocessed_data['Y'] = unprocessed_data['Y'].astype('float64') *unit_conversion_xy\n",
    "    unprocessed_data['Z'] = unprocessed_data['Z'].astype('float64') *unit_conversion_z\n",
    "\n",
    "# Pivot the DataFrame to create the 2D map\n",
    "    pivot_table = unprocessed_data.pivot_table(index='Y', columns='X', values='Z')\n",
    "    print('Pivoted the DataFrame')\n",
    "\n",
    "# Save the cleaned and pivoted DataFrame to the new file\n",
    "    np.savetxt(output_path, pivot_table, delimiter=\",\")\n",
    "\n",
    "    save_time = time.time()\n",
    "    elapsed_time = save_time - start_time\n",
    "    print(f\"Read, Convert, and Save time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "#------------#------------#\n",
    "# Step 2: Read the data\n",
    "#------------#------------#\n",
    "\n",
    "# Read data\n",
    "    A = pd.read_csv(output_path)\n",
    "\n",
    "# This grabs all the real data from our now 2D array\n",
    "    raw_data = A.iloc[:,1:]\n",
    "    \n",
    "#------------#------------# \n",
    "# Step 3: Find the mid point\n",
    "#------------#------------#\n",
    "\n",
    "#Find Edges\n",
    "    def find_edge(data, minimum_points): #Finds 'top' edge\n",
    "        data = pd.DataFrame(data)\n",
    "        for i in range(data.shape[0]): # shape gives [row, col]\n",
    "            data_slice = data.iloc[i, :].astype(np.float64) #slice of just one row\n",
    "            if np.isnan(data_slice.values).sum() < data.shape[1]-(min_points-1):\n",
    "                return i\n",
    "        \n",
    "    top_edge = find_edge(raw_data, min_points)\n",
    "    bottom_edge = raw_data.shape[0]-find_edge(np.flipud(raw_data), min_points)-1\n",
    "    left_edge = find_edge(np.transpose(raw_data), min_points)\n",
    "    right_edge = raw_data.shape[1] - find_edge(np.flipud(np.transpose(raw_data)), min_points)-1\n",
    "\n",
    "#Transpose to match physical specimen\n",
    "    raw_data = (pd.DataFrame(data=raw_data).T).astype(float)\n",
    "    raw_data = pd.DataFrame(data=raw_data)\n",
    "    raw_data = raw_data*1\n",
    "\n",
    "#Define sample centres: X & Y apparently swapped due to dataframe being transposed\n",
    "    Y0 = int(round(mean((left_edge, right_edge)),0)) #since the dataset has been transposed\n",
    "    X0 = int(round(mean((top_edge, bottom_edge)),0)) #since the dataset has been transposed\n",
    "\n",
    "#Define X & Y axes from dataset\n",
    "    X = (np.linspace(0, raw_data.shape[1]-1, raw_data.shape[1])-X0)*X_res/1000 # px\n",
    "    Y = (np.linspace(0, raw_data.shape[0]-1, raw_data.shape[0])-Y0)*Y_res/1000 # px\n",
    "\n",
    "#------------#------------#\n",
    "# Step 4: Cut off the edges of the data before filtering\n",
    "#------------#------------#\n",
    "\n",
    "#Removing edges using Y axis\n",
    "    edge_removal = 1*raw_data\n",
    "\n",
    "#Construction of outer and inner radii to remove edge effects\n",
    "\n",
    "#Outside of left outer edge (i.e. i<Xoutneg)\n",
    "    for i in range(0, int(X0-outer_radius)+1, 1):\n",
    "        edge_removal.iloc[i,:] = np.nan\n",
    "\n",
    "#Outside of right outer edge (i.e. i>Xoutneg)\n",
    "    for i in range(int(X0+outer_radius)+1, raw_data.shape[0], 1):\n",
    "        edge_removal.iloc[i,:] = np.nan\n",
    "\n",
    "#Outside of lower outer edge (i.e. i<Y_outer_negative)\n",
    "    for i in range(0, int(Y0-outer_radius)+1, 1):\n",
    "        edge_removal.iloc[:, i] = np.nan\n",
    "\n",
    "#Outside of upper outer edge (i.e. i>Y_outer_negative)\n",
    "    for i in range(int(Y0+outer_radius)+1, raw_data.shape[1], 1):\n",
    "        edge_removal.iloc[:, i] = np.nan\n",
    "     \n",
    "    \n",
    "#Between Xout and Xin (for both sides)\n",
    "    for i in range(int(X0-outer_radius)+1, int(X0)+1, 1):\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i, 0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "\n",
    "    for i in range(int(X0), int(X0+outer_radius), 1):\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "\n",
    "        edge_removal.iloc[i, 0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "\n",
    "#Between Xinneg and Xinpos (hole in middle as a single section)\n",
    "    for i in range(int(X0-inner_radius+1), int(X0+inner_radius-1), 1):\n",
    "    #Outer portion\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i,0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "        \n",
    "    #Inner portion\n",
    "        Y_inner_positive = int(Y0 + math.sqrt(inner_radius**2 - (i-X0)**2))\n",
    "        Y_inner_negative = int(Y0 - math.sqrt(inner_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i, Y_inner_negative:Y_inner_positive] = np.nan\n",
    "\n",
    "    edgeless_data = edge_removal\n",
    "\n",
    "#------------#------------#\n",
    "# Step 5: Filter noise\n",
    "#------------#------------#\n",
    "\n",
    "# Filtering of data\n",
    "# Excluding data outside of mean+x*standarddeviations, where x = std (below) and the mean is found using a moving average\n",
    "\n",
    "    def filter_from_moving_average(data, window, std_range):\n",
    "    # flaten to ease rolling averages\n",
    "        flat_data = data.to_numpy().flatten()\n",
    "        series_data = pd.Series(flat_data)\n",
    "    # take rolling mean and std of moving window width\n",
    "        rolling_mean = series_data.rolling(window=window, min_periods=1).mean()\n",
    "        rolling_std = series_data.rolling(window=window, min_periods=1).std()\n",
    "    \n",
    "        for i in range(window // 2, len(flat_data) - window // 2):\n",
    "            if abs(rolling_mean[i] - flat_data[i]) >= abs(std_range * rolling_std[i]): # if rolling average is out of standard deviation\n",
    "                flat_data[i] = np.nan\n",
    "        return pd.DataFrame(flat_data.reshape(data.shape))\n",
    "\n",
    "    filtered_edgeless_data = filter_from_moving_average(edgeless_data, moving_average, std_range)\n",
    "\n",
    "    filter_time = time.time()\n",
    "    print('Total Filtering Time (s): ', filter_time-start_time)\n",
    "    \n",
    "    \n",
    "#------------#------------#    \n",
    "# Step 6: Interpolate small to large in XY direction\n",
    "#------------#------------#\n",
    "\n",
    "#Interpolate across whole sample (without edges)\n",
    "    def interpolate_data(data):\n",
    "        interpolated = data.copy()\n",
    "        for limit in [15, 30, 60, 120, 250]:\n",
    "            interpolated = interpolated.interpolate(method='linear', axis=1, limit=limit)\n",
    "            interpolated = interpolated.interpolate(method='linear', axis=0, limit=limit)\n",
    "        return interpolated\n",
    "\n",
    "    interpolated_data = interpolate_data(filtered_edgeless_data)\n",
    "    \n",
    "#------------#------------#    \n",
    "#Step 6.5: Re-remove edge filled in due to interpolation\n",
    "#------------#------------#\n",
    "\n",
    "#Second edge removal\n",
    "    edge_removal = 1*interpolated_data\n",
    "\n",
    "#Outside of left outer edge (i.e. i<Xoutneg)\n",
    "    for i in range(0, int(X0-outer_radius)+1, 1):\n",
    "        edge_removal.iloc[i,:] = np.nan\n",
    "\n",
    "#Outside of right outer edge (i.e. i>Xoutneg)\n",
    "    for i in range(int(X0+outer_radius)+1, raw_data.shape[0], 1):\n",
    "        edge_removal.iloc[i,:] = np.nan\n",
    "\n",
    "    \n",
    "#Outside of lower outer edge (i.e. i<Y_outer_negative)\n",
    "    for i in range(0, int(Y0-outer_radius)+1, 1):\n",
    "        edge_removal.iloc[:, i] = np.nan\n",
    "\n",
    "#Outside of upper outer edge (i.e. i>Y_outer_negative)\n",
    "    for i in range(int(Y0+outer_radius)+1, raw_data.shape[1], 1):\n",
    "        edge_removal.iloc[:, i] = np.nan\n",
    "    \n",
    "    \n",
    "    \n",
    "#Between Xout and Xin (for both sides)\n",
    "    for i in range(int(X0-outer_radius)+1, int(X0)+1, 1):\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i, 0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "\n",
    "    for i in range(int(X0), int(X0+outer_radius), 1):\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "\n",
    "        edge_removal.iloc[i, 0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "\n",
    "#Between Xinneg and Xinpos (hole in middle as a single section)\n",
    "    for i in range(int(X0-inner_radius+1), int(X0+inner_radius-1), 1):\n",
    "    #Outer portion\n",
    "        Y_outer_positive = int(Y0 + math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "        Y_outer_negative = int(Y0 - math.sqrt(outer_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i,0:Y_outer_negative] = np.nan\n",
    "        edge_removal.iloc[i, Y_outer_positive:edge_removal.shape[1]-1] = np.nan\n",
    "        \n",
    "    #Inner portion\n",
    "        Y_inner_positive = int(Y0 + math.sqrt(inner_radius**2 - (i-X0)**2))\n",
    "        Y_inner_negative = int(Y0 - math.sqrt(inner_radius**2 - (i-X0)**2))\n",
    "    \n",
    "        edge_removal.iloc[i, Y_inner_negative:Y_inner_positive] = np.nan\n",
    "\n",
    "    edgeless_interpolated_data = edge_removal\n",
    "\n",
    "\n",
    "    interpolate_time = time.time()\n",
    "    print('Interpolation Time (s): ', interpolate_time-start_time)\n",
    "    \n",
    "#------------#------------#\n",
    "# Step 7: Tilt correction\n",
    "#------------#------------#\n",
    "\n",
    "\n",
    "    def tilt_correction(data, X, Y):\n",
    "    \n",
    "    # Tilt Correction\n",
    "        F = 1 * data\n",
    "        data_flat = np.ndarray.flatten(np.asarray(data))\n",
    "        data_mean = np.nanmean(data_flat)\n",
    "        data_std = np.nanstd(data_flat)\n",
    "    \n",
    "    # Remove data out of STD range\n",
    "        F[(F > data_mean + data_std)] = np.nan\n",
    "        F[(F < data_mean - data_std)] = np.nan\n",
    "    \n",
    "        G = 1 * F\n",
    "        f_flat = np.ndarray.flatten(np.asarray(F))\n",
    "        f_mean = np.nanmean(f_flat)\n",
    "        f_std = 2 * np.nanstd(f_flat)\n",
    "    \n",
    "    # Remove data out of STD range again\n",
    "        G[(G > f_mean + f_std)] = np.nan\n",
    "        G[(G < f_mean - f_std)] = np.nan\n",
    "        G_flat = np.ndarray.flatten(np.asarray(G))\n",
    "    \n",
    "        X_rep = np.tile(X, len(Y))\n",
    "        Y_rep = np.repeat(Y, len(X))\n",
    "    \n",
    "    # Finding X slope\n",
    "        LR_X = pd.DataFrame({'X_rep': X_rep, 'G_flat': G_flat})\n",
    "        LR_X_clean = LR_X.dropna(axis=0)\n",
    "        slope_x, intercept_x = np.polyfit(LR_X_clean['X_rep'], LR_X_clean['G_flat'], 1)\n",
    "    \n",
    "    # Correcting X tilt\n",
    "        X_corr = slope_x * X\n",
    "        data_X = data.subtract(X_corr, axis='columns')\n",
    "    \n",
    "    # Define X Tilt using correction factor\n",
    "        X_Tilt = math.cos(np.arctan(np.nanmean(slope_x) / 1000))\n",
    "    \n",
    "    # Finding Y slope\n",
    "        LR_Y = pd.DataFrame({'Y_rep': Y_rep, 'G_flat': G_flat})\n",
    "        LR_Y_clean = LR_Y.dropna(axis=0)\n",
    "        slope_y, intercept_y = np.polyfit(LR_Y_clean['Y_rep'], LR_Y_clean['G_flat'], 1)\n",
    "    \n",
    "    # Correcting Y tilt\n",
    "        Y_corr = slope_y * Y\n",
    "        data_YX = data_X.subtract(Y_corr, axis='rows')\n",
    "    \n",
    "    # Define Y Tilt using correction factor\n",
    "        Y_Tilt = math.cos(np.arctan(np.nanmean(slope_y) / 1000))\n",
    "    \n",
    "    # Define new X & Y positions\n",
    "        X_new = X / X_Tilt\n",
    "        Y_new = Y / Y_Tilt\n",
    "    \n",
    "        return data_YX, X_new, Y_new, X_Tilt, Y_Tilt\n",
    "\n",
    "#Tilt Correction 1\n",
    "    tilted_data_1, X_new, Y_new, X_Tilt_1, Y_Tilt_1 = tilt_correction(edgeless_interpolated_data, X, Y)\n",
    "\n",
    "    tilt_1_finish = time.time()\n",
    "    print('Time for first tilt correction (s): ', tilt_1_finish - start_time)\n",
    "    \n",
    "#Do it again Trust me bro\n",
    "#Tilt Correction 2\n",
    "    tilted_data_2, X_new, Y_new, X_Tilt_2, Y_Tilt_2 = tilt_correction(tilted_data_1, X_new, Y_new)\n",
    "\n",
    "#Do it again Trust me bro\n",
    "#Tilt Correction 3\n",
    "    tilted_data_3, X_new, Y_new, X_Tilt_3, Y_Tilt_3 = tilt_correction(tilted_data_2, X_new, Y_new)\n",
    "\n",
    "    tilt_3_finish = time.time()\n",
    "    print('Time for third tilt correction (s): ', tilt_3_finish - start_time)\n",
    "\n",
    "\n",
    "#------------#------------#\n",
    "# Step 8: Translating so that the most populus plane is at zero\n",
    "#------------#------------#\n",
    "\n",
    "#Initial translation so all data >=0, which makes finding the modal bin easier\n",
    "    data_min = np.amin(tilted_data_3.min()) #Getting the min hieght value\n",
    "    zerod_data = tilted_data_3-data_min #Translating so minimimum is at Z=0 \n",
    "\n",
    "#Fill in NaN cells as 0 (otherwise binning/histogram doesn't work)\n",
    "    filled_zerod_data = zerod_data.fillna(0)\n",
    "\n",
    "#Z-Correction: Finding the zero-plane and translating this\n",
    "    interval_bins = 2\n",
    "    A = np.ndarray.flatten(np.asarray(filled_zerod_data))\n",
    "    Amax = int(np.ceil(np.amax(A)))+50\n",
    "#Set bins. Don't want to start_time at 0 since this will now contain what were previously NaN elements (ALL of them!) \n",
    "    bins = list(range(interval_bins, Amax + interval_bins, interval_bins))\n",
    "    x, y = np.histogram(A, bins) #Need dataframe to be a 1D array (NOT 2D!!)\n",
    "\n",
    "\n",
    "#Finding POSITION of modal bin i.e. where should zero-plane be\n",
    "#NB: np.argmax(x) finds number of bin (NB: start_timeing at bin number 0) that max population is in\n",
    "    print('Position of modal bin (counts): ', interval_bins*np.argmax(x), '-', interval_bins*(np.argmax(x)+1))\n",
    "    print('Z-correction factor: ', interval_bins*(np.argmax(x)+0.5))\n",
    "\n",
    "\n",
    "#Correcting Z such that the modal plane is at zero\n",
    "    translated_data = zerod_data - interval_bins*(np.argmax(x)+0.5)\n",
    "\n",
    "    \n",
    "    Z_sort_nan = np.sort(np.ndarray.flatten(np.asarray(translated_data))) #Flattening the dataframe into a 1D array & ordering (doesn't matter but easy to do)\n",
    "    Z_sort = Z_sort_nan[~np.isnan(Z_sort_nan)] #Removing the NaN cells (enabling further operations to be performed), -ve sign to remove NaN\n",
    "\n",
    "#Finding the area of the data shown (i.e. not NaN), named 'Reduced Area' (RArea for short)\n",
    "#Size of each pixel for corrected sample\n",
    "    X_new_res = mean_res/(1000*X_Tilt_1*X_Tilt_2*X_Tilt_3) # mm\n",
    "    Y_new_res = mean_res/(1000*Y_Tilt_1*Y_Tilt_2*Y_Tilt_3) # mm\n",
    "    new_mean_res = (X_new_res + Y_new_res)/2  # mm\n",
    "    \n",
    "    R_Area = (1000*X_new_res)*(1000*Y_new_res)*len(Z_sort) #Multiplied by 1000 (twice) since Z is in um andd X/Ynewsize are in mm\n",
    "    \n",
    "#------------#------------#\n",
    "# Step 9: Save to HDF5 File so it won't have to be processed again\n",
    "#------------#------------#\n",
    "    \n",
    "    # pixel size given in mm^2\n",
    "    pixel_size = X_new_res * Y_new_res\n",
    "    print(f'Pixel size is {pixel_size:.4g} mm^2')\n",
    "\n",
    "# Save data to HDF5 file\n",
    "    with h5py.File(saving_h5_path, 'w') as f:\n",
    "        f.create_dataset('X_new', data=X_new)\n",
    "        f.create_dataset('Y_new', data=Y_new)\n",
    "        f.create_dataset('translated_data', data=translated_data)\n",
    "        f.create_dataset('raw_data', data=raw_data)\n",
    "        f.create_dataset('tilted_data_3', data=tilted_data_3)\n",
    "        f.attrs['pixel_size'] = pixel_size\n",
    "        \n",
    "    func_finish = time.time()\n",
    "    print('Time for full function (s): ', func_finish - start_time)\n",
    "\n",
    "#------------#------------#\n",
    "# Step 10: De-Bugging\n",
    "#------------#------------#  \n",
    "    \n",
    "    def check_if_good_processing():\n",
    "        print(f\"Raw Data (size: {raw_data.shape}): \", raw_data)\n",
    "        print('-----------------------------------------')\n",
    "        print(\"Left Edge:\", left_edge, \"Right Edge:\", right_edge, \n",
    "          \"Top Edge:\", top_edge, \"Bottom Edge:\", bottom_edge)\n",
    "        print('X0: ', X0)\n",
    "        print('Y0: ', Y0)\n",
    "        print('-----------------------------------------')\n",
    "        print('Moving Point Average / - : ', moving_average+1)\n",
    "        print('Number of Standard Deviations from Mean Allowed: ', std_range)\n",
    "        print('-----------------------------------------')\n",
    "        print('Filtered Max height / um: ', np.amax(filtered_edgeless_data.max()))\n",
    "        print('Filtered Min height / um: ', np.amin(filtered_edgeless_data.min()))\n",
    "        print('Edges Rt / um', np.amax(filtered_edgeless_data.max())-np.amin(filtered_edgeless_data.min()))\n",
    "        print('-----------------------------------------')\n",
    "        print('Corrected Edgeless Max height / um: ', np.amax(translated_data.max()))\n",
    "        print('Corrected Edgless Min height / um: ', np.amin(translated_data.min()))\n",
    "        print('Corrected Edgless Rt / um: ', np.amax(translated_data.max())-np.amin(translated_data.min()))\n",
    "        print('-------------------------------------------')\n",
    "        #------------#------------#\n",
    "        # Compare Raw Data and Processed Data\n",
    "        #------------#------------#\n",
    "\n",
    "        # Create a figure with two subplots\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Plot the original data on the first subplot\n",
    "        c1 = ax[0].contourf(X_new, Y_new, raw_data, 256, vmin=np.amin(raw_data), vmax=np.amax(raw_data))\n",
    "        ax[0].set_title('Original Data')\n",
    "        ax[0].axis('equal')\n",
    "        fig.colorbar(c1, ax=ax[0], ticks=range(-800, -100, 100), label='Z Position / um')\n",
    "        c1.set_clim(vmax=np.amax(raw_data), vmin=np.amin(raw_data))\n",
    "        ax[0].set_xlabel('X Position / mm')\n",
    "        ax[0].set_ylabel('Y Position / mm')\n",
    "        ax[0].spines[\"top\"].set_visible(False)\n",
    "        ax[0].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        # Plot the corrected data on the second subplot\n",
    "        norm_cor = TwoSlopeNorm(vmin=np.amin(translated_data), vcenter=0, vmax=np.amax(translated_data))\n",
    "        c2 = ax[1].contourf(X_new, Y_new, translated_data, 256, norm=norm_cor)\n",
    "        ax[1].set_title('Corrected Data')\n",
    "        ax[1].axis('equal')\n",
    "        fig.colorbar(c2, ax=ax[1], ticks=range(-500, 500, 50), label='Z Position / um')\n",
    "        c2.set_clim(vmax=np.amax(translated_data), vmin=np.amin(translated_data))\n",
    "        ax[1].set_xlabel('X Position / mm')\n",
    "        ax[1].set_ylabel('Y Position / mm')\n",
    "        ax[1].spines[\"top\"].set_visible(False)\n",
    "        ax[1].spines[\"right\"].set_visible(False)\n",
    "\n",
    "        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0) # Spread out graphs to avoid overlap\n",
    "\n",
    "        # Resizing the plot area\n",
    "        fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "        fig_size[0] = 12 # X-axis size\n",
    "        fig_size[1] = 12 # Y-axis size\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "# Use if needing to de-bug\n",
    "    '''check_if_good_processing()'''\n",
    "        \n",
    "#------------#------------#\n",
    "# End Function\n",
    "    #return X_new_res, Y_new_res, R_Area\n",
    "#------------#------------#\n",
    "\n",
    "# Run function\n",
    "process_raw_data(csv_path, output_path, saving_h5_path)\n",
    "\n",
    "print('Finished!')\n",
    "end = time.time()\n",
    "print('Time elapsed: ', end - start_time, ' seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a89ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
